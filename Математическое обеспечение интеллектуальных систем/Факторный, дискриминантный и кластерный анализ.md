---
date: 2024-10-21
tags:
  - theory
  - mathematical_support_of_intelligent_systems
---
Факторный анализ занимается определением значимости различных факторов на систему.

Снимаются показания переменных $x_{1},x_{2}\dots x_{n}$. Эти переменные зависят от различных факторов. $x_{i}=\sum\limits_{j=1}^{m}c_{ij}F_{j}+U_{i}+\delta_{i}$ - в факторном анализе принята линейная модель.

Факторный анализ разделяется на разведочный (поиск) и конфирматорный (проверка).

В факторном анализе предполагается, что наблюдаемые переменные являются линейной комбинацией некоторых латентных (гипотетических или ненаблюдаемых) факторов. Некоторые из этих факторов допускаются общими для двух и более переменных, а другие - характерными для каждого параметра в отдельности. Характерные факторы - ортогональны друг другу (по крайней мере в разведочном анализе). Следовательно, характерные факторы не вносят вклад в ковариацию между переменными. Другими словами, только общие факкторы, число которых предполагается гораздо меньшим числа наблюдаемых переменных, вносят вклад в ковариацию между ними.

Факторный анализ обычно состоит из трех шагов:
1. Подготовки соответствующей корреляционной матрицы
2. Выведения первоначальных (ортогональных) факторов
3. Вращения с целью получения окончательного решения

## Основные алгоритмы и методы факторного анализа
На первом этапе могут использоваться *модель общих факторов* и *анализ главных компонент*, цель которого отлична от факторного анализа. В то же время оба метода широко используются и являются эффективными способами исследования "взаимосвязей" между переменными. Основное отличие между этими двумя методами заключается в том, что главные компоненты являются линейными функциями от наблюдаемых переменных, в то время как общие факторы не выражаются через комбинацию наблюдаемых переменных. Альтернативой анализу первоначальных факторов служит анализ образов-факторов, в котором предполагается, что наблюдаемые переменные выбраны из бесконечного множества переменных, причем вводятся «образы-факторы», являющиеся линейными комбинациями переменных. Сопоставление этих подходов будет рассмотрено ниже. Кроме того, существует несколько путей выделения первоначальных факторов. Из них в этой работе рассматриваются следующие: 1) решение, получаемое методом максимального правдоподобия (включая канонический факторный анализ); 2) решение по методу наименших квадратов (включая метод минимальных остатков и метод главных факторов с итерациями по общностям) и 3) альфа-факторный анализ. Последний может рассматриваться либо как вариант метода с общими факторами, либо как альтернативная стратегия.

## Методы выделения первоначальных факторов
Основная цель выделения первичных факторов в разведочном факторном анализе заключается в определении минимального числа общих факторов, которые удовлетворительно воспроизводят корреляции между наблюдаемыми переменными. При отсутствии ошибок измерений и случайности в выборке, а также при выполнении принципа факторной причинности, для заданной корреляционной матрицы существует точное соответствие между минимальным числом общих факторов и рангом редуцированной корреляционной матрицы. (В редуцированной корреляционной матрице общности помещаются на главную диагональ.) Т.е., в случае отстутствия ошибок в соответствии факторной модели данных число общих факторов и общности могут быть сколь угодно точно вычислены с помощью исследования ранга редуцированной корреяционной матрицы.

## Главные компоненты, собственные значения и вектора
Мы начинаем обсуждение именно с анализа главных компонент по двум причинам: во-первых, он послужит в качестве базовой модели, с которой будут сравниваться и сопоставляться методы, где используются общие факторы, Во-вторых, он представляется наиболее простым для введения таких особых понятий, как корни характеристического уравнения (собственные числа) и собственные вектора, и дает возможность выявить их роли в алгоритмах факторного анализа.

Анализ главных компонент - это метод преобразования данной последовательности наблюдаемых переменных в другую последовательность переменных. Наиболее простой способ пояснить внутреннюю логику метода сводится к его изучению в двумерном случае. Предположим, что есть две переменные $X$ и $Y$ с совместным нормальным распределением.

![[IMG20241028131419.jpg|400]]
![[IMG20241028131446.jpg]]

### Методы вращения
Целью всех вращений является получение наиболее простой факторной структуры. Наиболее точное определение простой структуры дано Тэрстоуном, но не все его критерии формализуются в аналитическом методе. Мы остановимся на более простом подходе Мьюлейка, предполагающем знание лишь элементов теории векторных пространств.

Подходы к решению проблемы вращения:
- графический(геометрический)
  Проведение новых осей так, чтобы оси проходили через скопления точек.
  Практически неприменим, когда скопления точек трудноразличимы или число факторов больше двух.
- аналитический
  Выбирается объективный критерий, которым руководствуются при вращении. Различают ортогональное и косоугольное вращение.
- факторного отображения
  Цель фращения - нахождение факторного отображения, наиболее близкого к некоторой заданной матрице. Т.к. при задании целевой матрицы делаются определенные предположения о факторной структуре, третий подход схож с конфирматорным факторным анализом, в котором проверяются гипотезы о матрице факторного отображения.

Требования к простой структуре по Мьюлейку ($r$ - число факторов):
1. В каждой строке матрицы вторичной структуры $V$ должен быть хотя бы один нулевой элемент. Это предположение является основным в определении простой структуры.
2. Для каждого столбца $k$ матрицы вторичной структуры $V$ должно существовать подмножество из $r$ линейно-независимых наблюдаемых переменных, корреляции которых с $k$-ым вторичным фактором - нулевые. Данный критерий своддится к тому, что каждый столюец матрицы должен соержать не менее $r$ нулей.
3. У одного из столбцов каждой пары столбцов матрицы $V$ долно быть несколько нулевых коэффициентов (нагрузок) в тех позициях, где для другого столбца они ненулевые. Это предположение гарантирует различимость вторичных осей и соответствующих им подпространств размерности $r-1$ в пространстве общих факторов.
4. При числе общих факторов больше четырех в каждой паре столбцов должно быть некоторое количество нулевых нагрузок в одних и тех же строках. Данное предположение дает возможность разделить наблюдаемые переменные на отдельные скопления.
5. Для каждой пары столбцов матрицы $V$ должно быть как можно меньше значительных по величине нагрузок, соответствующих одним и тем же строкам. Это требование обеспечивает минимизацию сложности переменных.

#### Методы ортогонального вращения: квартимакс, варимакс, эквимакс
Число факторов $k$, число переменных $n$.

Одной  из мер сложности модели является вариация квадрата факторной нагрузки для каждой строки (для каждой переменной).

Факторная сложность $i$-той переменной = $\frac{1}{r}\sum\limits_{j=1}^{r}(b_{ij}^{2}-\overline{b^{2}_{ij}})^{2}$, где $r$ - число столбцов факторной матрицы, $b_{ij}$ - факторная нагрузка $j$-го фактора на $i$-ю переменную, $\overline{b_{ij}}$ - среднее значение квадратов факторных нагрузок в $i$-той строке.
Соотношение может быть предствалено в виде:$$q_{i}=\frac{\sum\limits_{j=1}^{r}(b_{ij}^{4})-(\sum\limits_{j=1}^{r}b_{ij}^{2})^{2}}{r^{2}}$$
Число фактороа $r$ и общности каждой переменной считаются известными в результате решения задачи выделения первоначальных факторов. Поэтому слагаемое, входящее в уравнение с отрицательным знаком, является константой ($\sum\limits_{j=1}^{r}b^{2}_{ij}=h_{i}^{2}$) в случае ортогонального решения.
Общей мерой сложности может служить сумма $q_{i}$ всех переменных.
Использование критерия **квартимакс** основано на вращении осей таким образом, чтобы результирующие факторные нагрузки максимизировали $q$. При этом максимализация $q$ эквивалентна максимизации выражения $Q=\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{r}b_{ij}$.

Метод **варимакс** использует другой критерий, по столбцам.$$v_{j}=\frac{n\sum\limits_{i=1}^{n}b_{ij}^{4}-(\sum\limits_{i=1}^{n}b_{ij}^{2})^{2}}{n^{2}}$$
Общая мера простоты задается критерием $V=\sum\limits_{j=1}^{r}v_{j}$.

Учитывая, что варимакс основан на упрощении столбцов, а квартимакс - упрощении строк, можно предположить совместный критерий, введя ссответствующие веса: $\alpha Q+\beta V=Maximum$.
$\sum\limits_{i=1}^{r}\sum\limits_{j=1}^{n}b_{ij}^{4}-\gamma \sum\limits_{j=1}^{r}(\sum\limits_{i=1}^{n}b_{ij}^{2})^{2}/n=Maximum$, $\gamma=\frac{\beta}{\alpha+\beta}$
Если $\gamma=1$ - образуется метод варимакс, если $\gamma=0$ - квартимакс.
При $\gamma=\frac{r}{2}$ и $\gamma=0.5$ получаем особые критерии, называемые соответственно эквимакс и биквартимакс.

#### Методы косоуголного вращения
Косоугольное вращение является более общим, чем ортогональное, т.к. здесь нет ограничений, связанных с некоррелированностью факторов.

> [!warning] 
> Далее конспект неполный, следует обратиться к первоисточнику (Факторный, дискриминантный и кластерный анализ)

Преимущество состоит в следующем: когда в результате его выполнения

##### Методы, основанные на введении вторичных осей
Если существуют разделимые скопления точек, определяемые первичными факторами, о они будут иметь почти нулевые проекции на все вторичные оси, за исключением одной. Таким образом, можно определить критерий, называемый квартимин, аналогичный квартимаксу:$$N=\sum\limits_{i=1}^{n}\sum\limits_{j<k=1}^{r}a_{ij}a_{ik}$$
Где $a_{ij},a_{ik}$ - проекции нагрузок

По аналогии с варимаксом - коваримин:$$C=\sum\limits_{j<k=1}$$
Их объединение - облимин:$$B=\alpha N+\beta C/n=minimum$$
где $\alpha,\beta$ - веса для $N,C$ соответственно. После умножения на $n$.

##### Вращение с использованием целевой матрицы


### Определение числа факторов
Хотя мы уже ранее рассмотрели ряд методов нахождения минимального числа факторов, обеспечиваюших согласие с наблюдениями, однако существуют причины, чтобы вернуться к этому вопросу.
Во-первых, при обсуждении метода выделения первоначальных факторов отмечалось, что число факторов можно оценивать достаточно приблизительно, поэтому мы не будем вдаваться в подробности, относящиеся к данной задаче.
Во-вторых, некоторые первоначальные решения не дают достоверной информации о числе факторов, так как требуют последующего проведения вращений.
В-третьих, мы можем столкнуться с затруднениями, связанными с неполным соответствием между факторной моделью и данными наблюдений.
В-четвертых, надо быть готовыми к тому, что в большинстве компьютерных программ требуется предварительныя оценка числа факторов.

Существует несколько часто употребляемых критериев определения числа факторов.
Наиболее часто применяются:
1. критерии знаимости, связанные с методами максимального правдоподобия и наименьших квадратов
2. различные правила, формулируемые в терминах собственных чисел
3. критерий, основанный на величине долей дисперсий фатокров
4. критерий отсеивания
5. критерий интерпретируемости и инвариантности

#### Критерии значимости
##### Критерии, основанные на собственных числах
При определении числа факторов часто применяют правило, которое позволяет оставлять факторы с собственными числами, большими 1. При этом используется корреляционная (нередуци- рованная) матрица. Этот простой критерий хорошо себя зареко- мендовал, так как обычно дает результаты, совпадающие с теми, что ожидает получить исследователь. Кроме того, этот метод был тщательно проверен на модельных искусственных данных.

Для корреляционной матрицы, относящейся к генеральной совокупности, рассматриваемый критерий всегда дает нижнюю оценку числа общих факторов. Иначе говоря, число общих фак- торов, соответствующих данной корреляционной матрице, будет больше или равно числу факторов, выделяемых согласно этому критерию. Однако полученное неравенство не обязательно спра- ведливо для выборочной корреляционной матрицы. Хотя Кайзер приводит несколько причин в пользу критерия собственных чи сел, больших 1, тем не менее он носит эвристический характер. После исследования других, более «утонченных» методов, Кайзер все же отдает предпочтение именно этому критерию (Kaiser, 1974).

Другой метод, основанный на собственных числах, относится к редуцированной корреляционной матрице. Согласно этому кри терию сохраняются факторы с собственными числами, большими нуля. Преимущество этого метода в том, что для корреляцион ной матрицы генеральной совокупности он дает более точные нижние оценки числа общих факторов. ов. Но для выборочной корреляционной матрицы критерий обычно дает значительно большее число факторов.

Данный критерий может применяться, когда общности оцениваются и помещаются на главную диагональ. Как правило, не которые собственные числа будут отрицательными. При этом не имеет смысла выделять все факторы с собственными числами, большими нуля. Хотя сумма отрицательных и положительных собственных чисел равна сумме всех общностей, (т. е. дисперсии, объясняемой общими факторами), отрицательные величины нельзя интерпретировать как дисперсии. Поэтому их присутствие яв ляется причиной «инфляции» суммы положительных собственных чисел в том смысле, что она становится больше суммы общностей. Харман (Harman, 1975) предлагает прекратить выделение общих факторов, когда сумма собственных чисел превысит сумму оценок общностей.

##### 
Возможен третий подход - для каждого фактора оценивается доля дисперсии, воспроизводимая этим фактором. Данный крите рий становится особенно наглядным, когда выделение первоначальных факторов производится корреляционной матрицы. Тогда с помощью нередуцированной в качестве статистики этого критерия выступает доля дисперсии, воспроизводимой последним выделяемым фактором по отношению к полной дисперсии, равной числу параметров. Следует напомнить, что рассмотренные выше методы выделения предполагают упорядочение факторов по убыванию их долей дисперсии. Обсуждаемый критерий опре деляется уровнем (порогом) для минимальной доли воспроизводимой дисперсии. Например, это может быть один, пять или десять процентов. Заметим, что критерий «собственных чисел, боль- пих единицы», эквивалентен данному критерию для 100/n%-го уровня.

##### Критерий отсеивания
Предложен Кателлом. Рассматривается графическое изображение собственных чисел корреляционной матрицы, которые наносятся на график в порядке их убывания. Выделение заканчивается на том факторе, после которого исследуемая зависимость близка к почти горизонтальной прямой линии. Эту прямую Кателл и предлагает использовать для выделения факторов.

##### КРИТЕРИЙ ИНТЕРПРЕТИРУЕМОСТИ И ИНВАРИАНТНОСТИ
Для исключения сомнительных результатов можно попытаться применить к одним и тем же данным комбинацию различных независимых критериев и принимать только те результаты, которые подходят ко всем критериям (Наrris, 1967). Окончательное решение должно базироваться на его приемлемости с точки зре ния научных представлений в данной области. Этот подход является «обходным маневром», но, к сожалению, а может быть и к счастью, мы вынуждены принять его, если хотим, чтобы нашими результатами могли воспользоваться другие исследователи.
